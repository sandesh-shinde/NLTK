{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Neural Network and Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data source : IMDB Movie Reviews\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 50,000 movie reviews <br>\n",
    "* 25000 reviews for training and 25000 for testing classifier <br>\n",
    "* Each set has 12,500 positive and 12,500 negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = tarfile.open('data/S_Classification/aclImdb_v1.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  tf.extractall(path='data/S_Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('data/S_Classification/aclImdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use load_files module from sklearn to load train and test datasets in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'data/S_Classification/aclImdb/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = r'data/S_Classification/aclImdb/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_train = load_files(train_dir, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_test = load_files(test_dir, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check path of filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['data/S_Classification/aclImdb/train\\\\neg\\\\0_3.txt',\n",
       "       'data/S_Classification/aclImdb/train\\\\neg\\\\10000_4.txt',\n",
       "       'data/S_Classification/aclImdb/train\\\\neg\\\\10001_4.txt', ...,\n",
       "       'data/S_Classification/aclImdb/train\\\\pos\\\\999_10.txt',\n",
       "       'data/S_Classification/aclImdb/train\\\\pos\\\\99_8.txt',\n",
       "       'data/S_Classification/aclImdb/train\\\\pos\\\\9_7.txt'], dtype='<U52')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_train.filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check target sentiment label (dependent variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_train.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check target folder names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check length of training data , i.e. 12,500 positive and 12,500 negative. Total = 25,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movies_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check 1st review for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_train.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check target label of 1st review , neg = 0 , pos = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_train.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_train.target_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = movies_train.data\n",
    "reviews_test = movies_test.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert reviews from byte format to string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_str = [str(i, encoding='utf') for i in reviews_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test_str = [str(j, encoding='utf') for j in reviews_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_no_space = re.compile(r\"[.;:!\\'?,\\\"()\\[\\]]\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_with_space = re.compile(r\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function to clean the Text data by removing punctuations etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reviews(reviews):\n",
    "    reviews = [replace_no_space.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [replace_with_space.sub(\" \", line) for line in reviews]\n",
    "    \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_clean = preprocess_reviews(reviews_train_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test_clean = preprocess_reviews(reviews_test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "\n",
    "###  Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                      if word not in english_stop_words])\n",
    "        )\n",
    "    return removed_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_words_train = remove_stop_words(reviews_train_clean)\n",
    "no_stop_words_test = remove_stop_words(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmed_text(corpus):\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_reviews_train = get_stemmed_text(no_stop_words_train)\n",
    "stemmed_reviews_test = get_stemmed_text(no_stop_words_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatized_text(corpus):\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_reviews_train = get_lemmatized_text(no_stop_words_train)\n",
    "lemmatized_reviews_test = get_lemmatized_text(no_stop_words_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set target_label variable to be 0 for negative reviews(first 12,500) and 1 for positive reviews(last 12,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = [0 if i < 12500 else 1 for i in range(25000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "\n",
    "### Bag of Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ngram\n",
    "\n",
    "* Add more predictive power to our model by adding two or three word sequences (bigrams or trigrams). <br>\n",
    "* For example, if a review had the three word sequence “didn’t love movie” we would only consider these words individually with a unigram-only model and probably not capture that this is actually a negative sentiment because the word ‘love’ by itself is going to be highly correlated with a positive review. <br>\n",
    "* We use unigram and bigram in our analysis with the ngram_range = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit BOW vectorizer on Lemmatized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_ngram_vectorizer.fit(lemmatized_reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow = bow_ngram_vectorizer.transform(lemmatized_reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bow = bow_ngram_vectorizer.transform(lemmatized_reviews_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check Vocabulary size and Number of features after BOW Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1802180\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(bow_ngram_vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1802180\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features: {}\".format(len(bow_ngram_vectorizer.get_feature_names()))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit TF-IDF Vectorizer on Lemmatized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.fit(lemmatized_reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tfidf_vectorizer.transform(lemmatized_reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_vectorizer.transform(lemmatized_reviews_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check Vocabulary size and Number of features after TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 83953\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(tfidf_vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 83953\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features: {}\".format(len(tfidf_vectorizer.get_feature_names()))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary size and Number of Features for BOW Vectorizer = 1802180 \n",
    "#### Vocabulary size and Number of Features for TF-IDF Vectorizer = 83953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tfidf = target_label.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_tfidf = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model_tfidf.fit(X_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,roc_auc_score \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pred = nn_model_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Neural Network Multi-layer Perceptron classifier : 0.8506\n",
      "The classification report is : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85     13065\n",
      "           1       0.83      0.87      0.85     11935\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.85     25000\n",
      "weighted avg       0.85      0.85      0.85     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of Neural Network Multi-layer Perceptron classifier :\" , accuracy_score(nn_pred, target_label)) \n",
    "print(\"The classification report is : \\n\"+classification_report(nn_pred, target_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_mat_nn = confusion_matrix(nn_pred, target_label)\n",
    "fig_nn, ax_nn = plot_confusion_matrix(conf_mat=conf_mat_nn,figsize=(5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_nn_tfidf = pickle.dumps(nn_model_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Word2Vec from Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_train = lemmatized_reviews_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(lem_train):\n",
    "    tokenized_1 = []\n",
    "    for word in sentence.split(' '):\n",
    "        word = word.split(',')[0]\n",
    "        tokenized_1.append(word)\n",
    "    lem_train[i] = tokenized_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['story', 'man', 'unnatural', 'feeling', 'pig', 'start', 'opening', 'scene', 'terrific', 'example', 'absurd', 'comedy', 'formal', 'orchestra', 'audience', 'turned', 'insane', 'violent', 'mob', 'crazy', 'chanting', 'singer', 'unfortunately', 'stay', 'absurd', 'whole', 'time', 'general', 'narrative', 'eventually', 'making', 'putting', 'even', 'era', 'turned', 'cryptic', 'dialogue', 'would', 'make', 'shakespeare', 'seem', 'easy', 'third', 'grader', 'technical', 'level', 'better', 'might', 'think', 'good', 'cinematography', 'future', 'great', 'vilmos', 'zsigmond', 'future', 'star', 'sally', 'kirkland', 'frederic', 'forrest', 'seen', 'briefly']\n"
     ]
    }
   ],
   "source": [
    "print(lem_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(lem_train, workers = 3, size = 50, min_count = 1, window = 3, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exceptional', 0.9105460047721863),\n",
       " ('outstanding', 0.9096971750259399),\n",
       " ('marvelous', 0.9092910289764404),\n",
       " ('excellent', 0.9059677124023438),\n",
       " ('great', 0.9000802040100098),\n",
       " ('fabulous', 0.8975570201873779),\n",
       " ('fine', 0.8904979228973389),\n",
       " ('marvellous', 0.889639139175415),\n",
       " ('superlative', 0.8894743919372559),\n",
       " ('fantastic', 0.8834149241447449)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('terrific')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrible', 0.9570490121841431),\n",
       " ('awful', 0.9393264055252075),\n",
       " ('abysmal', 0.935506284236908),\n",
       " ('horrendous', 0.927490234375),\n",
       " ('dreadful', 0.9203273057937622),\n",
       " ('horrid', 0.9187706708908081),\n",
       " ('atrocious', 0.9127136468887329),\n",
       " ('wretched', 0.9009042382240295),\n",
       " ('dire', 0.8820762634277344),\n",
       " ('sucked', 0.8804844617843628)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('terrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8286703"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.similarity('great','awesome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33597535"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.similarity('worst','chair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 86475\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(word2vec_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_test = lemmatized_reviews_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(lem_test):\n",
    "    tokenized_2 = []\n",
    "    for word in sentence.split(' '):\n",
    "        word = word.split(',')[0]\n",
    "        tokenized_2.append(word)\n",
    "    lem_test[i] = tokenized_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr', 'costner', 'dragged', 'movie', 'far', 'longer', 'necessary', 'aside', 'terrific', 'sea', 'rescue', 'sequence', 'care', 'character', 'u', 'ghost', 'closet', 'costners', 'character', 'realized', 'early', 'forgotten', 'much', 'later', 'time', 'care', 'character', 'really', 'care', 'cocky', 'overconfident', 'ashton', 'kutcher', 'problem', 'come', 'kid', 'think', 'he', 'better', 'anyone', 'else', 'around', 'show', 'sign', 'cluttered', 'closet', 'obstacle', 'appears', 'winning', 'costner', 'finally', 'well', 'past', 'half', 'way', 'point', 'stinker', 'costner', 'tell', 'u', 'kutchers', 'ghost', 'told', 'kutcher', 'driven', 'best', 'prior', 'inkling', 'foreshadowing', 'magic', 'could', 'keep', 'turning', 'hour']\n"
     ]
    }
   ],
   "source": [
    "print(lem_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate sentence vectors using the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for sentence in lem_train:\n",
    "    sentence_vectors = np.array([word2vec_model[token] for token in sentence])\n",
    "    mean_values = np.mean(sentence_vectors, axis=0)\n",
    "res.append(mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.38104385, -0.44952896,  0.25337642,  0.32768905, -0.00606357,\n",
       "        0.09828613,  0.01879919,  0.35495302, -0.31736574,  0.17353198,\n",
       "        0.3165656 , -0.27051362,  0.21134481, -0.3051421 ,  0.10668591,\n",
       "        0.19478637,  0.31447473,  0.14214855, -0.79690915, -0.30942753,\n",
       "        0.10540593,  0.02995077, -0.02301975, -0.41130224, -0.22889072,\n",
       "       -0.17975736,  0.01317331, -0.31977576,  0.19489793, -0.1821324 ,\n",
       "       -0.14876673,  0.42580447,  0.3380368 ,  0.2716853 ,  0.41068748,\n",
       "        0.15025261,  0.06449486, -0.26747286, -0.2613322 ,  0.37052867,\n",
       "        0.04148499,  0.14894351, -0.4384459 , -0.59139276, -0.13014454,\n",
       "       -0.02076569, -0.11877716, -0.5947032 ,  0.10769217,  0.41534463],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
