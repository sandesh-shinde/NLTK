{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NLTK is a powerful Python package that provides a set of diverse natural languages processing algorithms. <br> \n",
    "* It is free, opensource, easy to use, large community, and well documented. <br> \n",
    "* With NLTK, we can do tokenizing, part-of-speech tagging, stemming, sentiment analysis, topic segmentation, and named entity recognition. <br> \n",
    "* NLTK helps the computer to do analysis, preprocess, and understand the written text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install NLTK before using it. It can be installed with the help of the following command  <br>\n",
    "* pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then download the NLTK packages for Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenization is the first step in text analytics. <br>\n",
    "* It is the process of breaking down a text paragraph into smaller chunks such as words or sentences. <br>\n",
    "* Token is a single entity that is a building block for a sentence or paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization <br>\n",
    "Sentence tokenizer breaks text paragraph into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example , I have taken the following text from a student review on Macquarie University for the Bachelor of Speech and Hearing Science course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\" Overall, I enjoyed my course and time at Macquarie Uni. The course was extremely relevant and insightful into what career I wanted to pursue, but heavily lacked the practical element.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overall, I enjoyed my course and time at Macquarie Uni. The course was extremely relevant and insightful into what career I wanted to pursue, but heavily lacked the practical element.\n"
     ]
    }
   ],
   "source": [
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the sent_tokenize package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Overall, I enjoyed my course and time at Macquarie Uni.', 'The course was extremely relevant and insightful into what career I wanted to pursue, but heavily lacked the practical element.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenizer breaks text paragraph into words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the word_tokenize package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Overall', ',', 'I', 'enjoyed', 'my', 'course', 'and', 'time', 'at', 'Macquarie', 'Uni', '.', 'The', 'course', 'was', 'extremely', 'relevant', 'and', 'insightful', 'into', 'what', 'career', 'I', 'wanted', 'to', 'pursue', ',', 'but', 'heavily', 'lacked', 'the', 'practical', 'element', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stop words are words which occur frequently in a corpus. <br>\n",
    "* Stop words are considered as noise in the text as they don't add any value in Text Analysis <br>\n",
    "* Examples of Stop words : is, am, are, this, a, an, the, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLTK for removing stopwords, we need to create a list of stopwords and filter out our list of tokens from these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'which', 'while', \"you'd\", 'during', 'himself', 'too', 'myself', 'll', 'd', \"aren't\", \"haven't\", 'him', 'don', 'when', \"you've\", 'over', 'she', 'being', 'or', 'them', 'couldn', 'his', \"mustn't\", 'what', 'where', 'wouldn', 'once', 'we', 'up', 'hadn', 'same', 'themselves', 'whom', 'more', 'been', \"wasn't\", 'these', 'that', 'y', 'off', 'did', 'aren', 'about', 'other', 'if', 'shan', \"isn't\", \"won't\", \"you'll\", \"doesn't\", 'through', 'each', 'above', 'of', 'between', 'our', 'very', \"weren't\", 'be', \"don't\", 'most', 'me', 've', 'mustn', 'under', 'should', 'o', 'down', 'now', 'own', 'my', 'the', \"hasn't\", 'to', 're', 'on', 'how', 'few', 'further', \"hadn't\", 'itself', 'yourself', 'herself', 'her', 'doing', 'its', 'can', \"she's\", 'ours', 'theirs', 'why', 'nor', 'who', 'their', 'am', 'mightn', 'you', 'until', 't', 'i', 'shouldn', 'ma', 'won', 'below', \"shan't\", 'hasn', 'were', \"shouldn't\", \"wouldn't\", 'have', 'some', 'a', 'but', 'any', 'will', 'as', 'had', 'this', 'having', 'all', 'do', 'yourselves', 'then', 'does', 'was', 'just', 'isn', 'from', 'didn', 'an', \"needn't\", \"it's\", 'no', 'not', 'with', 'ain', 'those', 'there', 'again', 'both', \"should've\", 'haven', 'has', 'doesn', 'for', 'is', 'ourselves', 'he', 'only', \"that'll\", 'against', 'out', \"didn't\", \"couldn't\", 'here', 'in', 'm', 'yours', 'after', 'because', 'it', \"mightn't\", 'by', 'wasn', 's', 'and', 'at', 'they', 'needn', 'your', 'hers', \"you're\", 'into', 'before', 'than', 'are', 'so', 'weren', 'such'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the Text paragraph by removing the Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = []\n",
    "for word in tokenized_words :\n",
    "    if word not in stop_words :\n",
    "        filtered_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Overall', ',', 'I', 'enjoyed', 'course', 'time', 'Macquarie', 'Uni', '.', 'The', 'course', 'extremely', 'relevant', 'insightful', 'career', 'I', 'wanted', 'pursue', ',', 'heavily', 'lacked', 'practical', 'element', '.']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stemming is a process of linguistic normalization, which reduces words to their word root word or chops off the derivational affixes <br>\n",
    "* It is reduction of inflection from words. Words with same origin will get reduced to a form which may or may not be a word "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the Stemming packages are PorterStemmer, LancasterStemmer and SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use PorterStemmer for our use case and import the library as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = []\n",
    "for word in filtered_words :\n",
    "    stemmed_words.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['overal', ',', 'I', 'enjoy', 'cours', 'time', 'macquari', 'uni', '.', 'the', 'cours', 'extrem', 'relev', 'insight', 'career', 'I', 'want', 'pursu', ',', 'heavili', 'lack', 'practic', 'element', '.']\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lemmatization reduces words to their base word, which is linguistically correct lemmas. <br>\n",
    "* It transforms the root word with the use of vocabulary and morphological analysis. <br> \n",
    "* Lemmatization is usually more sophisticated than stemming. <br>\n",
    "* Stemmer works on an individual word without knowledge of the context. For example, The word \"better\" has \"good\" as its lemma.This thing will get missed by stemming because it requires a dictionary look-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the WordNetLemmatizer package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = []\n",
    "for word in filtered_words :\n",
    "    lemmatized_words.append(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Overall', ',', 'I', 'enjoyed', 'course', 'time', 'Macquarie', 'Uni', '.', 'The', 'course', 'extremely', 'relevant', 'insightful', 'career', 'I', 'wanted', 'pursue', ',', 'heavily', 'lacked', 'practical', 'element', '.']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech(POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group or parts of speech of a given word.<br>\n",
    "* Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERB, etc. based on the context. <br>\n",
    "* POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokens = nltk.pos_tag(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Overall', 'JJ'), (',', ','), ('I', 'PRP'), ('enjoyed', 'VBP'), ('my', 'PRP$'), ('course', 'NN'), ('and', 'CC'), ('time', 'NN'), ('at', 'IN'), ('Macquarie', 'NNP'), ('Uni', 'NNP'), ('.', '.'), ('The', 'DT'), ('course', 'NN'), ('was', 'VBD'), ('extremely', 'RB'), ('relevant', 'JJ'), ('and', 'CC'), ('insightful', 'JJ'), ('into', 'IN'), ('what', 'WP'), ('career', 'NN'), ('I', 'PRP'), ('wanted', 'VBD'), ('to', 'TO'), ('pursue', 'VB'), (',', ','), ('but', 'CC'), ('heavily', 'RB'), ('lacked', 'VBD'), ('the', 'DT'), ('practical', 'JJ'), ('element', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms cannot work with raw text directly; the text must be converted into numbers. Specifically, vectors of numbers. The vectors x are derived from textual data, in order to reflect various linguistic properties of the text. This is called feature extraction or feature encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. <br> \n",
    "* In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. <br> \n",
    "* The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier. <br> \n",
    "* A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:<br> 1) A vocabulary of known words. <br> 2) A measure of the presence of known words.<br> \n",
    "* It is called a “bag” of words, because any information about the order or structure of words in the document is discarded.<br> \n",
    "* The model is only concerned with whether known words occur in the document, not where in the document.The intuition is that documents are similar if they have similar content. Further, that from the content alone we can learn something about the meaning of the document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps <br> \n",
    "* We iterate through each sentence in the corpus(paragraph), convert the sentence to lower case, and then remove the punctuation and empty spaces from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Regular Expression module\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].lower()\n",
    "    corpus[i] = re.sub(r'\\W',' ',corpus[i])\n",
    "    corpus[i] = re.sub(r'\\s+',' ',corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' overall i enjoyed my course and time at macquarie uni ', 'the course was extremely relevant and insightful into what career i wanted to pursue but heavily lacked the practical element ']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The next step is to tokenize the words in the sentences and create a dictionary that contains words and their corresponding frequencies in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': 1,\n",
       " 'i': 2,\n",
       " 'enjoyed': 1,\n",
       " 'my': 1,\n",
       " 'course': 2,\n",
       " 'and': 2,\n",
       " 'time': 1,\n",
       " 'at': 1,\n",
       " 'macquarie': 1,\n",
       " 'uni': 1,\n",
       " 'the': 2,\n",
       " 'was': 1,\n",
       " 'extremely': 1,\n",
       " 'relevant': 1,\n",
       " 'insightful': 1,\n",
       " 'into': 1,\n",
       " 'what': 1,\n",
       " 'career': 1,\n",
       " 'wanted': 1,\n",
       " 'to': 1,\n",
       " 'pursue': 1,\n",
       " 'but': 1,\n",
       " 'heavily': 1,\n",
       " 'lacked': 1,\n",
       " 'practical': 1,\n",
       " 'element': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The final step is to convert the sentences in our corpus into their corresponding vector representation. <br> \n",
    "* For each word in the wordfreq dictionary if the word exists in the sentence, a 1 will be added for the word, else 0 will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    for token in wordfreq:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vector_array = np.asarray(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vector_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Model\n",
    "### Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One of the main problems associated with the bag of words model is that it assigns equal value to the words, irrespective of their importance. <br>\n",
    "* The words that are rare have more classifying power compared to the words that are common. <br>\n",
    "* The idea behind the TF-IDF approach is that the words that are more common in one sentence and less common in other sentences should be given high weights.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDF: log((Total number of sentences (documents))/(Number of sentences (documents) containing the word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idf_values = {}\n",
    "for token in wordfreq:\n",
    "    doc_containing_word = 0\n",
    "    for document in corpus:\n",
    "        if token in nltk.word_tokenize(document):\n",
    "            doc_containing_word += 1\n",
    "    word_idf_values[token] = np.log(len(corpus)/(1 + doc_containing_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': 0.0,\n",
       " 'i': -0.40546510810816444,\n",
       " 'enjoyed': 0.0,\n",
       " 'my': 0.0,\n",
       " 'course': -0.40546510810816444,\n",
       " 'and': -0.40546510810816444,\n",
       " 'time': 0.0,\n",
       " 'at': 0.0,\n",
       " 'macquarie': 0.0,\n",
       " 'uni': 0.0,\n",
       " 'the': 0.0,\n",
       " 'was': 0.0,\n",
       " 'extremely': 0.0,\n",
       " 'relevant': 0.0,\n",
       " 'insightful': 0.0,\n",
       " 'into': 0.0,\n",
       " 'what': 0.0,\n",
       " 'career': 0.0,\n",
       " 'wanted': 0.0,\n",
       " 'to': 0.0,\n",
       " 'pursue': 0.0,\n",
       " 'but': 0.0,\n",
       " 'heavily': 0.0,\n",
       " 'lacked': 0.0,\n",
       " 'practical': 0.0,\n",
       " 'element': 0.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idf_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The next step is to create the TF dictionary for each word. <br>\n",
    "* In the TF dictionary, the key will be the most frequently occuring words, while values will be 'N' dimensional vectors, where 'N' is the number of sentences. <br> \n",
    "* Each value in the vector will belong to the TF value of the word for the corresponding sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tf_values = {}\n",
    "for token in wordfreq:\n",
    "    sent_tf_vector = []\n",
    "    for document in corpus:\n",
    "        doc_freq = 0\n",
    "        for word in nltk.word_tokenize(document):\n",
    "            if token == word:\n",
    "                  doc_freq += 1\n",
    "        word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
    "        sent_tf_vector.append(word_tf)\n",
    "    word_tf_values[token] = sent_tf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': [0.1, 0.0],\n",
       " 'i': [0.1, 0.05],\n",
       " 'enjoyed': [0.1, 0.0],\n",
       " 'my': [0.1, 0.0],\n",
       " 'course': [0.1, 0.05],\n",
       " 'and': [0.1, 0.05],\n",
       " 'time': [0.1, 0.0],\n",
       " 'at': [0.1, 0.0],\n",
       " 'macquarie': [0.1, 0.0],\n",
       " 'uni': [0.1, 0.0],\n",
       " 'the': [0.0, 0.1],\n",
       " 'was': [0.0, 0.05],\n",
       " 'extremely': [0.0, 0.05],\n",
       " 'relevant': [0.0, 0.05],\n",
       " 'insightful': [0.0, 0.05],\n",
       " 'into': [0.0, 0.05],\n",
       " 'what': [0.0, 0.05],\n",
       " 'career': [0.0, 0.05],\n",
       " 'wanted': [0.0, 0.05],\n",
       " 'to': [0.0, 0.05],\n",
       " 'pursue': [0.0, 0.05],\n",
       " 'but': [0.0, 0.05],\n",
       " 'heavily': [0.0, 0.05],\n",
       " 'lacked': [0.0, 0.05],\n",
       " 'practical': [0.0, 0.05],\n",
       " 'element': [0.0, 0.05]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tf_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have IDF values of all the words, along with TF values of every word across the sentences. <br>\n",
    "* The next step is to simply multiply IDF values with TF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_values = []\n",
    "for token in word_tf_values.keys():\n",
    "    tfidf_sentences = []\n",
    "    for tf_sentence in word_tf_values[token]:\n",
    "        tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "        tfidf_sentences.append(tf_idf_score)\n",
    "    tfidf_values.append(tfidf_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model = np.asarray(tfidf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ],\n",
       "       [-0.04054651, -0.02027326],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [-0.04054651, -0.02027326],\n",
       "       [-0.04054651, -0.02027326],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, each column represents the TF-IDF vector for the corresponding sentence. <br>\n",
    "We want rows (instead of Columns) to represent the TF-IDF vectors, so we transpose our numpy array as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model_t = np.transpose(tf_idf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.04054651,  0.        ,  0.        , -0.04054651,\n",
       "        -0.04054651,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        , -0.02027326,  0.        ,  0.        , -0.02027326,\n",
       "        -0.02027326,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_model_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though TF-IDF is an improvement over the simple bag of words approach and yields better results for common NLP tasks, we still need to create a huge sparse matrix, which also takes a lot more computation than the simple bag of words approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec approach uses deep learning and neural networks-based techniques to convert words into corresponding vectors in such a way that the semantically similar vectors are close to each other in N-dimensional space, where N refers to the dimensions of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word2Vec retains the semantic meaning of different words in a document.The context information is not lost. <br>\n",
    "* Another great advantage of Word2Vec approach is that the size of the embedding vector is very small. Each dimension in the embedding vector contains information about one aspect of the word. <br> \n",
    "* We do not need huge sparse vectors, unlike the bag of words and TF-IDF approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We import Gensim library to create Word2Vec model. <br> \n",
    "* The word list is passed to the Word2Vec class of the gensim.models package. <br> \n",
    "* We need to specify the value for the min_count parameter. A value of 1 for min_count specifies to include only those words in the Word2Vec model that appear at least once in the corpus.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(filtered_words, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the dictionary of unique words that exist at least once in the corpus, execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = word2vec.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words = word2vec.wv.most_similar('I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', 0.21362608671188354),\n",
       " ('j', 0.2060253620147705),\n",
       " ('m', 0.1966397911310196),\n",
       " ('w', 0.13187554478645325),\n",
       " ('M', 0.13053743541240692),\n",
       " ('t', 0.10804124176502228),\n",
       " ('y', 0.08303216099739075),\n",
       " ('u', 0.05196388438344002),\n",
       " ('s', 0.030994432047009468),\n",
       " ('a', 0.0288662351667881)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
